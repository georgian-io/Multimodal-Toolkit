<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>multimodal_transformers.model &mdash; Multimodal Transformers  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="multimodal_transformers.data" href="data.html" />
    <link rel="prev" title="Colab Example" href="../notes/colab_example.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Multimodal Transformers
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notes/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/introduction.html">Introduction by Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/combine_methods.html">Combine Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/colab_example.html">Colab Example</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package Reference</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">multimodal_transformers.model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-multimodal_transformers.model.tabular_combiner">Tabular Feature Combiner</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#multimodal_transformers.model.tabular_combiner.TabularFeatCombiner"><code class="docutils literal notranslate"><span class="pre">TabularFeatCombiner</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#multimodal_transformers.model.tabular_combiner.TabularFeatCombiner.forward"><code class="docutils literal notranslate"><span class="pre">TabularFeatCombiner.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-multimodal_transformers.model.tabular_config">Tabular Config</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#multimodal_transformers.model.tabular_config.TabularConfig"><code class="docutils literal notranslate"><span class="pre">TabularConfig</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-multimodal_transformers.model.tabular_modeling_auto">AutoModel with Tabular</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#multimodal_transformers.model.tabular_modeling_auto.AutoModelWithTabular"><code class="docutils literal notranslate"><span class="pre">AutoModelWithTabular</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#multimodal_transformers.model.tabular_modeling_auto.AutoModelWithTabular.from_config"><code class="docutils literal notranslate"><span class="pre">AutoModelWithTabular.from_config()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#multimodal_transformers.model.tabular_modeling_auto.AutoModelWithTabular.from_pretrained"><code class="docutils literal notranslate"><span class="pre">AutoModelWithTabular.from_pretrained()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#module-multimodal_transformers.model.tabular_transformers">Transformers with Tabular</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.AlbertWithTabular"><code class="docutils literal notranslate"><span class="pre">AlbertWithTabular</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.AlbertWithTabular.forward"><code class="docutils literal notranslate"><span class="pre">AlbertWithTabular.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.BertWithTabular"><code class="docutils literal notranslate"><span class="pre">BertWithTabular</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.BertWithTabular.forward"><code class="docutils literal notranslate"><span class="pre">BertWithTabular.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.DistilBertWithTabular"><code class="docutils literal notranslate"><span class="pre">DistilBertWithTabular</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.DistilBertWithTabular.forward"><code class="docutils literal notranslate"><span class="pre">DistilBertWithTabular.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.LongformerWithTabular"><code class="docutils literal notranslate"><span class="pre">LongformerWithTabular</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.LongformerWithTabular.forward"><code class="docutils literal notranslate"><span class="pre">LongformerWithTabular.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.RobertaWithTabular"><code class="docutils literal notranslate"><span class="pre">RobertaWithTabular</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.RobertaWithTabular.forward"><code class="docutils literal notranslate"><span class="pre">RobertaWithTabular.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.XLMRobertaWithTabular"><code class="docutils literal notranslate"><span class="pre">XLMRobertaWithTabular</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.XLMRobertaWithTabular.config_class"><code class="docutils literal notranslate"><span class="pre">XLMRobertaWithTabular.config_class</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.XLMWithTabular"><code class="docutils literal notranslate"><span class="pre">XLMWithTabular</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.XLMWithTabular.forward"><code class="docutils literal notranslate"><span class="pre">XLMWithTabular.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.XLNetWithTabular"><code class="docutils literal notranslate"><span class="pre">XLNetWithTabular</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.XLNetWithTabular.forward"><code class="docutils literal notranslate"><span class="pre">XLNetWithTabular.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="data.html">multimodal_transformers.data</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Multimodal Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">multimodal_transformers.model</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/modules/model.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="multimodal-transformers-model">
<h1>multimodal_transformers.model<a class="headerlink" href="#multimodal-transformers-model" title="Link to this heading"></a></h1>
<nav class="contents local" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#module-multimodal_transformers.model.tabular_combiner" id="id2">Tabular Feature Combiner</a></p></li>
<li><p><a class="reference internal" href="#module-multimodal_transformers.model.tabular_config" id="id3">Tabular Config</a></p></li>
<li><p><a class="reference internal" href="#module-multimodal_transformers.model.tabular_modeling_auto" id="id4">AutoModel with Tabular</a></p></li>
<li><p><a class="reference internal" href="#module-multimodal_transformers.model.tabular_transformers" id="id5">Transformers with Tabular</a></p></li>
</ul>
</nav>
<section id="module-multimodal_transformers.model.tabular_combiner">
<span id="tabular-feature-combiner"></span><h2><a class="toc-backref" href="#id2" role="doc-backlink">Tabular Feature Combiner</a><a class="headerlink" href="#module-multimodal_transformers.model.tabular_combiner" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_combiner.TabularFeatCombiner">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">TabularFeatCombiner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">tabular_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_combiner.html#TabularFeatCombiner"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_combiner.TabularFeatCombiner" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></p>
<blockquote>
<div><p>Combiner module for combining text features with categorical and numerical features
The methods of combining, specified by <code class="xref py py-obj docutils literal notranslate"><span class="pre">tabular_config.combine_feat_method</span></code> are shown below.
<span class="math notranslate nohighlight">\(\mathbf{m}\)</span> denotes the combined multimodal features,
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> denotes the output text features from the transformer,
<span class="math notranslate nohighlight">\(\mathbf{c}\)</span> denotes the categorical features, <span class="math notranslate nohighlight">\(\mathbf{t}\)</span> denotes the numerical features,
<span class="math notranslate nohighlight">\(h_{\mathbf{\Theta}}\)</span> denotes a MLP parameterized by <span class="math notranslate nohighlight">\(\Theta\)</span>, <span class="math notranslate nohighlight">\(W\)</span> denotes a weight matrix,
and <span class="math notranslate nohighlight">\(b\)</span> denotes a scalar bias</p>
<ul>
<li><p><strong>text_only</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathbf{m} = \mathbf{x}\]</div>
</div></blockquote>
</li>
<li><p><strong>concat</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathbf{m} = \mathbf{x} \, \Vert \, \mathbf{c} \, \Vert \, \mathbf{n}\]</div>
</div></blockquote>
</li>
<li><p><strong>mlp_on_categorical_then_concat</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathbf{m} = \mathbf{x} \, \Vert \, h_{\mathbf{\Theta}}( \mathbf{c}) \, \Vert \, \mathbf{n}\]</div>
</div></blockquote>
</li>
<li><p><strong>individual_mlps_on_cat_and_numerical_feats_then_concat</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathbf{m} = \mathbf{x} \, \Vert \, h_{\mathbf{\Theta_c}}( \mathbf{c}) \, \Vert \, h_{\mathbf{\Theta_n}}(\mathbf{n})\]</div>
</div></blockquote>
</li>
<li><p><strong>mlp_on_concatenated_cat_and_numerical_feats_then_concat</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathbf{m} = \mathbf{x} \, \Vert \, h_{\mathbf{\Theta}}( \mathbf{c} \, \Vert \, \mathbf{n})\]</div>
</div></blockquote>
</li>
<li><p><strong>attention_on_cat_and_numerical_feats</strong> self attention on the text features</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathbf{m} = \alpha_{x,x}\mathbf{W}_x\mathbf{x} + \alpha_{x,c}\mathbf{W}_c\mathbf{c} + \alpha_{x,n}\mathbf{W}_n\mathbf{n}\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(\mathbf{W}_x\)</span> is of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(out_dim,</span> <span class="pre">text_feat_dim)</span></code>,
<span class="math notranslate nohighlight">\(\mathbf{W}_c\)</span> is of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(out_dim,</span> <span class="pre">cat_feat_dim)</span></code>,
<span class="math notranslate nohighlight">\(\mathbf{W}_n\)</span> is of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(out_dim,</span> <span class="pre">num_feat_dim)</span></code>, and the attention coefficients <span class="math notranslate nohighlight">\(\alpha_{i,j}\)</span> are computed as</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\alpha_{i,j} =
\frac{
\exp\left(\mathrm{LeakyReLU}\left(\mathbf{a}^{\top}
[\mathbf{W}_i\mathbf{x}_i \, \Vert \, \mathbf{W}_j\mathbf{x}_j]
\right)\right)}
{\sum_{k \in \{ x, c, n \}}
\exp\left(\mathrm{LeakyReLU}\left(\mathbf{a}^{\top}
[\mathbf{W}_i\mathbf{x}_i \, \Vert \, \mathbf{W}_k\mathbf{x}_k]
\right)\right)}.\]</div>
</div></blockquote>
</li>
<li><p><strong>gating_on_cat_and_num_feats_then_sum</strong> sum of features gated by text features. Inspired by the gating mechanism introduced in <a class="reference external" href="https://www.aclweb.org/anthology/2020.acl-main.214.pdf">Integrating Multimodal Information in Large Pretrained Transformers</a></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathbf{m}= \mathbf{x} + \alpha\mathbf{h}\]</div>
<div class="math notranslate nohighlight">
\[\mathbf{h} = \mathbf{g_c} \odot (\mathbf{W}_c\mathbf{c}) + \mathbf{g_n} \odot (\mathbf{W}_n\mathbf{n}) + b_h\]</div>
<div class="math notranslate nohighlight">
\[\alpha = \mathrm{min}( \frac{\| \mathbf{x} \|_2}{\| \mathbf{h} \|_2}*\beta, 1)\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(\beta\)</span> is a hyperparamter, <span class="math notranslate nohighlight">\(\mathbf{W}_c\)</span> is of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(out_dim,</span> <span class="pre">cat_feat_dim)</span></code>,
<span class="math notranslate nohighlight">\(\mathbf{W}_n\)</span> is of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(out_dim,</span> <span class="pre">num_feat_dim)</span></code>. and the gating vector <span class="math notranslate nohighlight">\(\mathbf{g}_i\)</span> with activation function <span class="math notranslate nohighlight">\(R\)</span> is defined as</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathbf{g}_i = R(\mathbf{W}_{gi}[\mathbf{i} \, \Vert \, \mathbf{x}]+ b_i)\]</div>
</div></blockquote>
<p>where <span class="math notranslate nohighlight">\(\mathbf{W}_{gi}\)</span> is of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(out_dim,</span> <span class="pre">i_feat_dim</span> <span class="pre">+</span> <span class="pre">text_feat_dim)</span></code></p>
</li>
<li><p><strong>weighted_feature_sum_on_transformer_cat_and_numerical_feats</strong></p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\mathbf{m} = \mathbf{x} + \mathbf{W}_{c'} \odot \mathbf{W}_c \mathbf{c} + \mathbf{W}_{n'} \odot \mathbf{W}_n \mathbf{t}\]</div>
</div></blockquote>
</li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>tabular_config</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">TabularConfig</span></code>) – Tabular model configuration class with all the parameters of the model.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_combiner.TabularFeatCombiner.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">text_feats</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">numerical_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_combiner.html#TabularFeatCombiner.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_combiner.TabularFeatCombiner.forward" title="Link to this definition"></a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>text_feats</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">text_out_dim)</span></code>) – The tensor of text features. This is assumed to be the output from a HuggingFace transformer model</p></li>
<li><p><strong>cat_feats</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">cat_feat_dim)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>)) – The tensor of categorical features</p></li>
<li><p><strong>numerical_feats</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">numerical_feat_dim)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – The tensor of numerical features</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor representing the combined features</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">final_out_dim)</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-multimodal_transformers.model.tabular_config">
<span id="tabular-config"></span><h2><a class="toc-backref" href="#id3" role="doc-backlink">Tabular Config</a><a class="headerlink" href="#module-multimodal_transformers.model.tabular_config" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_config.TabularConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">TabularConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_labels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_division</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">combine_feat_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'text_only'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_dropout</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">numerical_bn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">categorical_bn</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_simple_classifier</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_act</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gating_beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">numerical_feat_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat_feat_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_config.html#TabularConfig"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_config.TabularConfig" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Config used for tabular combiner</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mlp_division</strong> (<em>int</em>) – how much to decrease each MLP dim for each additional layer</p></li>
<li><p><strong>combine_feat_method</strong> (<em>str</em>) – The method to combine categorical and numerical features.
See <code class="xref py py-obj docutils literal notranslate"><span class="pre">TabularFeatCombiner</span></code> for details on the supported methods.</p></li>
<li><p><strong>mlp_dropout</strong> (<em>float</em>) – dropout ratio used for MLP layers</p></li>
<li><p><strong>numerical_bn</strong> (<em>bool</em>) – whether to use batchnorm on numerical features</p></li>
<li><p><strong>categorical_bn</strong> (<em>bool</em>) – whether to use batchnorm on categorical features</p></li>
<li><p><strong>use_simple_classifier</strong> (<em>bool</em>) – whether to use single layer or MLP as final classifier</p></li>
<li><p><strong>mlp_act</strong> (<em>str</em>) – the activation function to use for finetuning layers</p></li>
<li><p><strong>gating_beta</strong> (<em>float</em>) – <p>the beta hyperparameters used for gating tabular data
see the paper <a class="reference external" href="https://www.aclweb.org/anthology/2020.acl-main.214.pdf">Integrating Multimodal Information in Large Pretrained Transformers</a> for details</p>
</p></li>
<li><p><strong>numerical_feat_dim</strong> (<em>int</em>) – the number of numerical features</p></li>
<li><p><strong>cat_feat_dim</strong> (<em>int</em>) – the number of categorical features</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-multimodal_transformers.model.tabular_modeling_auto">
<span id="automodel-with-tabular"></span><h2><a class="toc-backref" href="#id4" role="doc-backlink">AutoModel with Tabular</a><a class="headerlink" href="#module-multimodal_transformers.model.tabular_modeling_auto" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_modeling_auto.AutoModelWithTabular">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">AutoModelWithTabular</span></span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_modeling_auto.html#AutoModelWithTabular"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_modeling_auto.AutoModelWithTabular" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_modeling_auto.AutoModelWithTabular.from_config">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_modeling_auto.html#AutoModelWithTabular.from_config"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_modeling_auto.AutoModelWithTabular.from_config" title="Link to this definition"></a></dt>
<dd><p>Instantiates one of the base model classes of the library
from a configuration.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Only the models in multimodal_transformers.py are implemented</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code>) – <dl class="simple">
<dt>The model class to instantiate is selected based on the configuration class:</dt><dd><p>see multimodal_transformers.py for supported transformer models</p>
</dd>
</dl>
</p>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>    <span class="c1"># Download configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithTabular</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_modeling_auto.AutoModelWithTabular.from_pretrained">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained_model_name_or_path</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">model_args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_modeling_auto.html#AutoModelWithTabular.from_pretrained"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_modeling_auto.AutoModelWithTabular.from_pretrained" title="Link to this definition"></a></dt>
<dd><p>Instantiates one of the sequence classification model classes of the library
from a pre-trained model configuration.
See multimodal_transformers.py for supported transformer models</p>
<p>The <cite>from_pretrained()</cite> method takes care of returning the correct model class instance
based on the <cite>model_type</cite> property of the config object, or when it’s missing,
falling back to using pattern matching on the <cite>pretrained_model_name_or_path</cite> string:</p>
<p>The model is set in evaluation mode by default using <cite>model.eval()</cite> (Dropout modules are deactivated)
To train the model, you should first set it back in training mode with <cite>model.train()</cite></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_model_name_or_path</strong> – <p>either:</p>
<ul>
<li><p>a string with the <cite>shortcut name</cite> of a pre-trained model to load from cache or download, e.g.: <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>.</p></li>
<li><p>a string with the <cite>identifier name</cite> of a pre-trained model that was user-uploaded to our S3, e.g.: <code class="docutils literal notranslate"><span class="pre">dbmdz/bert-base-german-cased</span></code>.</p></li>
<li><p>a path to a <cite>directory</cite> containing model weights saved using <code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code>, e.g.: <code class="docutils literal notranslate"><span class="pre">./my_model_directory/</span></code>.</p></li>
<li><p>a path or url to a <cite>tensorflow index checkpoint file</cite> (e.g. <cite>./tf_model/model.ckpt.index</cite>). In this case, <code class="docutils literal notranslate"><span class="pre">from_tf</span></code> should be set to True and a configuration object should be provided as <code class="docutils literal notranslate"><span class="pre">config</span></code> argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</p></li>
</ul>
</p></li>
<li><p><strong>model_args</strong> – (<cite>optional</cite>) Sequence of positional arguments:
All remaining positional arguments will be passed to the underlying model’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method</p></li>
<li><p><strong>config</strong> – <p>(<cite>optional</cite>) instance of a class derived from <code class="xref py py-class docutils literal notranslate"><span class="pre">PretrainedConfig</span></code>:
Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:</p>
<ul>
<li><p>the model is a model provided by the library (loaded with the <code class="docutils literal notranslate"><span class="pre">shortcut-name</span></code> string of a pretrained model), or</p></li>
<li><p>the model was saved using <code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code> and is reloaded by suppling the save directory.</p></li>
<li><p>the model is loaded by suppling a local directory as <code class="docutils literal notranslate"><span class="pre">pretrained_model_name_or_path</span></code> and a configuration JSON file named <cite>config.json</cite> is found in the directory.</p></li>
</ul>
</p></li>
<li><p><strong>state_dict</strong> – (<cite>optional</cite>) dict:
an optional state dictionary for the model to use instead of a state dictionary loaded from saved weights file.
This option can be used if you want to create a model from a pretrained configuration but load your own weights.
In this case though, you should check if using <code class="xref py py-func docutils literal notranslate"><span class="pre">save_pretrained()</span></code> and <code class="xref py py-func docutils literal notranslate"><span class="pre">from_pretrained()</span></code> is not a simpler option.</p></li>
<li><p><strong>cache_dir</strong> – (<cite>optional</cite>) string:
Path to a directory in which a downloaded pre-trained model
configuration should be cached if the standard cache should not be used.</p></li>
<li><p><strong>force_download</strong> – (<cite>optional</cite>) boolean, default False:
Force to (re-)download the model weights and configuration files and override the cached versions if they exists.</p></li>
<li><p><strong>resume_download</strong> – (<cite>optional</cite>) boolean, default False:
Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.</p></li>
<li><p><strong>proxies</strong> – (<cite>optional</cite>) dict, default None:
A dictionary of proxy servers to use by protocol or endpoint, e.g.: {‘http’: ‘foo.bar:3128’, ‘http://hostname’: ‘foo.bar:4012’}.
The proxies are used on each request.</p></li>
<li><p><strong>output_loading_info</strong> – (<cite>optional</cite>) boolean:
Set to <code class="docutils literal notranslate"><span class="pre">True</span></code> to also return a dictionary containing missing keys, unexpected keys and error messages.</p></li>
<li><p><strong>kwargs</strong> – (<cite>optional</cite>) Remaining dictionary of keyword arguments:
These arguments will be passed to the configuration and the model.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithTabular</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>    <span class="c1"># Download model and configuration from S3 and cache.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithTabular</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./test/bert_model/&#39;</span><span class="p">)</span>  <span class="c1"># E.g. model was saved using `save_pretrained(&#39;./test/saved_model/&#39;)`</span>
<span class="k">assert</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">output_attention</span> <span class="o">==</span> <span class="kc">True</span>
<span class="c1"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_json_file</span><span class="p">(</span><span class="s1">&#39;./tf_model/bert_tf_model_config.json&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelWithTabular</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;./tf_model/bert_tf_checkpoint.ckpt.index&#39;</span><span class="p">,</span> <span class="n">from_tf</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</section>
<section id="module-multimodal_transformers.model.tabular_transformers">
<span id="transformers-with-tabular"></span><h2><a class="toc-backref" href="#id5" role="doc-backlink">Transformers with Tabular</a><a class="headerlink" href="#module-multimodal_transformers.model.tabular_transformers" title="Link to this heading"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.AlbertWithTabular">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">AlbertWithTabular</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hf_model_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_transformers.html#AlbertWithTabular"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.AlbertWithTabular" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertForSequenceClassification</span></code></p>
<p>ALBERT Model transformer with a sequence classification/regression head as well as
a TabularFeatCombiner module to combine categorical and numerical features
with the Roberta pooled output</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>hf_model_config</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">AlbertConfig</span></code>) – Model configuration class with all the parameters of the model.
This object must also have a tabular_config member variable that is a
<code class="xref py py-obj docutils literal notranslate"><span class="pre">TabularConfig</span></code> instance specifying the configs for <code class="xref py py-obj docutils literal notranslate"><span class="pre">TabularFeatCombiner</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.AlbertWithTabular.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_type_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">position_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs_embeds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">numerical_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_transformers.html#AlbertWithTabular.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.AlbertWithTabular.forward" title="Link to this definition"></a></dt>
<dd><p>The [<cite>AlbertWithTabular</cite>] forward method, overrides the <cite>__call__</cite> special method.</p>
<p>&lt;Tip&gt;</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the [<cite>Module</cite>]
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.</p>
<p>&lt;/Tip&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>({0})</cite>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<cite>AutoTokenizer</cite>]. See [<cite>PreTrainedTokenizer.__call__</cite>] and
[<cite>PreTrainedTokenizer.encode</cite>] for details.</p>
<p>[What are input IDs?](../glossary#input-ids)</p>
</p></li>
<li><p><strong>attention_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>({0})</cite>, <em>optional</em>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p>[What are attention masks?](../glossary#attention-mask)</p>
</p></li>
<li><p><strong>token_type_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>({0})</cite>, <em>optional</em>) – <p>Segment token indices to indicate first and second portions of the inputs. Indices are selected in <cite>[0,
1]</cite>:</p>
<ul>
<li><p>0 corresponds to a <em>sentence A</em> token,</p></li>
<li><p>1 corresponds to a <em>sentence B</em> token.</p></li>
</ul>
<p>[What are token type IDs?](../glossary#token-type-ids)</p>
</p></li>
<li><p><strong>position_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>({0})</cite>, <em>optional</em>) – <p>Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <cite>[0,
config.max_position_embeddings - 1]</cite>.</p>
<p>[What are position IDs?](../glossary#position-ids)</p>
</p></li>
<li><p><strong>head_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>(num_heads,)</cite> or <cite>(num_layers, num_heads)</cite>, <em>optional</em>) – <p>Mask to nullify selected heads of the self-attention modules. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>inputs_embeds</strong> (<cite>torch.FloatTensor</cite> of shape <cite>({0}, hidden_size)</cite>, <em>optional</em>) – Optionally, instead of passing <cite>input_ids</cite> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors than the
model’s internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the attentions tensors of all attention layers. See <cite>attentions</cite> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the hidden states of all layers. See <cite>hidden_states</cite> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return a [<cite>~utils.ModelOutput</cite>] instead of a plain tuple.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the sequence classification/regression loss.
Indices should be in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.num_labels</span> <span class="pre">-</span> <span class="pre">1]</span></code>.
If <code class="docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">==</span> <span class="pre">1</span></code> a regression loss is computed (Mean-Square loss),
If <code class="docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> a classification loss is computed (Cross-Entropy).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.BertWithTabular">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">BertWithTabular</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hf_model_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_transformers.html#BertWithTabular"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.BertWithTabular" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">BertForSequenceClassification</span></code></p>
<p>Bert Model transformer with a sequence classification/regression head as well as
a TabularFeatCombiner module to combine categorical and numerical features
with the Bert pooled output</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>hf_model_config</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">BertConfig</span></code>) – Model configuration class with all the parameters of the model.
This object must also have a tabular_config member variable that is a
<code class="xref py py-obj docutils literal notranslate"><span class="pre">TabularConfig</span></code> instance specifying the configs for <code class="xref py py-obj docutils literal notranslate"><span class="pre">TabularFeatCombiner</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.BertWithTabular.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_type_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">position_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs_embeds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">numerical_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_transformers.html#BertWithTabular.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.BertWithTabular.forward" title="Link to this definition"></a></dt>
<dd><p>The [<cite>BertWithTabular</cite>] forward method, overrides the <cite>__call__</cite> special method.</p>
<p>&lt;Tip&gt;</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the [<cite>Module</cite>]
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.</p>
<p>&lt;/Tip&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<cite>AutoTokenizer</cite>]. See [<cite>PreTrainedTokenizer.encode</cite>] and
[<cite>PreTrainedTokenizer.__call__</cite>] for details.</p>
<p>[What are input IDs?](../glossary#input-ids)</p>
</p></li>
<li><p><strong>attention_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>, <em>optional</em>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p>[What are attention masks?](../glossary#attention-mask)</p>
</p></li>
<li><p><strong>token_type_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>, <em>optional</em>) – <p>Segment token indices to indicate first and second portions of the inputs. Indices are selected in <cite>[0,
1]</cite>:</p>
<ul>
<li><p>0 corresponds to a <em>sentence A</em> token,</p></li>
<li><p>1 corresponds to a <em>sentence B</em> token.</p></li>
</ul>
<p>[What are token type IDs?](../glossary#token-type-ids)</p>
</p></li>
<li><p><strong>position_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>, <em>optional</em>) – <p>Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <cite>[0,
config.max_position_embeddings - 1]</cite>.</p>
<p>[What are position IDs?](../glossary#position-ids)</p>
</p></li>
<li><p><strong>head_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>(num_heads,)</cite> or <cite>(num_layers, num_heads)</cite>, <em>optional</em>) – <p>Mask to nullify selected heads of the self-attention modules. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>inputs_embeds</strong> (<cite>torch.FloatTensor</cite> of shape <cite>((batch_size, sequence_length), hidden_size)</cite>, <em>optional</em>) – Optionally, instead of passing <cite>input_ids</cite> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors than the
model’s internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the attentions tensors of all attention layers. See <cite>attentions</cite> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the hidden states of all layers. See <cite>hidden_states</cite> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<cite>bool</cite>, <em>optional</em>) – <p>Whether or not to return a [<cite>~utils.ModelOutput</cite>] instead of a plain tuple.</p>
<dl class="simple">
<dt>labels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>):</dt><dd><p>Labels for computing the sequence classification/regression loss.
Indices should be in <code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.num_labels</span> <span class="pre">-</span> <span class="pre">1]</span></code>.
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">tabular_config.num_labels</span> <span class="pre">==</span> <span class="pre">1</span></code> a regression loss is computed (Mean-Square loss),
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">tabular_config.num_labels</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> a classification loss is computed (Cross-Entropy).</p>
</dd>
<dt>cat_feats (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">tabular_config.cat_feat_dim)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>):</dt><dd><p>Categorical features to be passed in to the TabularFeatCombiner</p>
</dd>
<dt>numerical_feats (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">tabular_config.numerical_feat_dim)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>):</dt><dd><p>Numerical features to be passed in to the TabularFeatCombiner</p>
</dd>
</dl>
</p></li>
<li><p><strong>Returns</strong> – <p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code> comprising various elements depending on configuration and inputs:
loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">label</span></code> is provided):</p>
<blockquote>
<div><p>Classification (or regression if tabular_config.num_labels==1) loss.</p>
</div></blockquote>
<dl class="simple">
<dt>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">tabular_config.num_labels)</span></code>):</dt><dd><p>Classification (or regression if tabular_config.num_labels==1) scores (before SoftMax).</p>
</dd>
<dt>classifier_layer_outputs(<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>):</dt><dd><p>The outputs of each layer of the final classification layers. The 0th index of this list is the
combining module’s output</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.DistilBertWithTabular">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">DistilBertWithTabular</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hf_model_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_transformers.html#DistilBertWithTabular"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.DistilBertWithTabular" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertForSequenceClassification</span></code></p>
<p>DistilBert Model transformer with a sequence classification/regression head as well as
a TabularFeatCombiner module to combine categorical and numerical features
with the Roberta pooled output</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>hf_model_config</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">DistilBertConfig</span></code>) – Model configuration class with all the parameters of the model.
This object must also have a tabular_config member variable that is a
<code class="xref py py-obj docutils literal notranslate"><span class="pre">TabularConfig</span></code> instance specifying the configs for <code class="xref py py-obj docutils literal notranslate"><span class="pre">TabularFeatCombiner</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.DistilBertWithTabular.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs_embeds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">numerical_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_transformers.html#DistilBertWithTabular.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.DistilBertWithTabular.forward" title="Link to this definition"></a></dt>
<dd><p>The [<cite>DistilBertWithTabular</cite>] forward method, overrides the <cite>__call__</cite> special method.</p>
<p>&lt;Tip&gt;</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the [<cite>Module</cite>]
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.</p>
<p>&lt;/Tip&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<cite>AutoTokenizer</cite>]. See [<cite>PreTrainedTokenizer.encode</cite>] and
[<cite>PreTrainedTokenizer.__call__</cite>] for details.</p>
<p>[What are input IDs?](../glossary#input-ids)</p>
</p></li>
<li><p><strong>attention_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>, <em>optional</em>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p>[What are attention masks?](../glossary#attention-mask)</p>
</p></li>
<li><p><strong>head_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>(num_heads,)</cite> or <cite>(num_layers, num_heads)</cite>, <em>optional</em>) – <p>Mask to nullify selected heads of the self-attention modules. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>inputs_embeds</strong> (<cite>torch.FloatTensor</cite> of shape <cite>((batch_size, sequence_length), hidden_size)</cite>, <em>optional</em>) – Optionally, instead of passing <cite>input_ids</cite> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors than the
model’s internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the attentions tensors of all attention layers. See <cite>attentions</cite> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the hidden states of all layers. See <cite>hidden_states</cite> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<cite>bool</cite>, <em>optional</em>) – <p>Whether or not to return a [<cite>~utils.ModelOutput</cite>] instead of a plain tuple.</p>
<dl class="simple">
<dt>labels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>):</dt><dd><p>Labels for computing the sequence classification/regression loss.
Indices should be in <code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.num_labels</span> <span class="pre">-</span> <span class="pre">1]</span></code>.
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">tabular_config.num_labels</span> <span class="pre">==</span> <span class="pre">1</span></code> a regression loss is computed (Mean-Square loss),
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">tabular_config.num_labels</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> a classification loss is computed (Cross-Entropy).</p>
</dd>
<dt>cat_feats (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">tabular_config.cat_feat_dim)</span></code>,`optional`, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>):</dt><dd><p>Categorical features to be passed in to the TabularFeatCombiner</p>
</dd>
<dt>numerical_feats (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">tabular_config.numerical_feat_dim)</span></code>,`optional`, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>):</dt><dd><p>Numerical features to be passed in to the TabularFeatCombiner</p>
</dd>
</dl>
</p></li>
<li><p><strong>Returns</strong> – <p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code> comprising various elements depending on configuration and inputs:
loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">label</span></code> is provided):</p>
<blockquote>
<div><p>Classification (or regression if tabular_config.num_labels==1) loss.</p>
</div></blockquote>
<dl class="simple">
<dt>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">tabular_config.num_labels)</span></code>):</dt><dd><p>Classification (or regression if tabular_config.num_labels==1) scores (before SoftMax).</p>
</dd>
<dt>classifier_layer_outputs(<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>):</dt><dd><p>The outputs of each layer of the final classification layers. The 0th index of this list is the
combining module’s output</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.LongformerWithTabular">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">LongformerWithTabular</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hf_model_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_transformers.html#LongformerWithTabular"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.LongformerWithTabular" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">LongformerForSequenceClassification</span></code></p>
<p>Longformer Model With Sequence Classification Head</p>
<dl class="py method">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.LongformerWithTabular.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_type_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">position_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs_embeds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">numerical_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_transformers.html#LongformerWithTabular.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.LongformerWithTabular.forward" title="Link to this definition"></a></dt>
<dd><p>The [<cite>LongformerWithTabular</cite>] forward method, overrides the <cite>__call__</cite> special method.</p>
<p>&lt;Tip&gt;</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the [<cite>Module</cite>]
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.</p>
<p>&lt;/Tip&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<cite>AutoTokenizer</cite>]. See [<cite>PreTrainedTokenizer.encode</cite>] and
[<cite>PreTrainedTokenizer.__call__</cite>] for details.</p>
<p>[What are input IDs?](../glossary#input-ids)</p>
</p></li>
<li><p><strong>attention_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>, <em>optional</em>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p>[What are attention masks?](../glossary#attention-mask)</p>
</p></li>
<li><p><strong>global_attention_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>, <em>optional</em>) – <p>Mask to decide the attention given on each token, local attention or global attention. Tokens with global
attention attends to all other tokens, and all other tokens attend to them. This is important for
task-specific finetuning because it makes the model more flexible at representing the task. For example,
for classification, the &lt;s&gt; token should be given global attention. For QA, all question tokens should also
have global attention. Please refer to the [Longformer paper](<a class="reference external" href="https://arxiv.org/abs/2004.05150">https://arxiv.org/abs/2004.05150</a>) for more
details. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>0 for local attention (a sliding window attention),</p></li>
<li><p>1 for global attention (tokens that attend to all other tokens, and all other tokens attend to them).</p></li>
</ul>
</p></li>
<li><p><strong>head_mask</strong> (<cite>torch.Tensor</cite> of shape <cite>(num_layers, num_heads)</cite>, <em>optional</em>) – <p>Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>decoder_head_mask</strong> (<cite>torch.Tensor</cite> of shape <cite>(num_layers, num_heads)</cite>, <em>optional</em>) – <p>Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>token_type_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>, <em>optional</em>) – <p>Segment token indices to indicate first and second portions of the inputs. Indices are selected in <cite>[0,
1]</cite>:</p>
<ul>
<li><p>0 corresponds to a <em>sentence A</em> token,</p></li>
<li><p>1 corresponds to a <em>sentence B</em> token.</p></li>
</ul>
<p>[What are token type IDs?](../glossary#token-type-ids)</p>
</p></li>
<li><p><strong>position_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>, <em>optional</em>) – <p>Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <cite>[0,
config.max_position_embeddings - 1]</cite>.</p>
<p>[What are position IDs?](../glossary#position-ids)</p>
</p></li>
<li><p><strong>inputs_embeds</strong> (<cite>torch.FloatTensor</cite> of shape <cite>((batch_size, sequence_length), hidden_size)</cite>, <em>optional</em>) – Optionally, instead of passing <cite>input_ids</cite> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors than the
model’s internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the attentions tensors of all attention layers. See <cite>attentions</cite> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the hidden states of all layers. See <cite>hidden_states</cite> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return a [<cite>~utils.ModelOutput</cite>] instead of a plain tuple.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.RobertaWithTabular">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">RobertaWithTabular</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hf_model_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_transformers.html#RobertaWithTabular"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.RobertaWithTabular" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaForSequenceClassification</span></code></p>
<p>Roberta Model transformer with a sequence classification/regression head as well as
a TabularFeatCombiner module to combine categorical and numerical features
with the Roberta pooled output</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>hf_model_config</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaConfig</span></code>) – Model configuration class with all the parameters of the model.
This object must also have a tabular_config member variable that is a
<code class="xref py py-obj docutils literal notranslate"><span class="pre">TabularConfig</span></code> instance specifying the configs for <code class="xref py py-obj docutils literal notranslate"><span class="pre">TabularFeatCombiner</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.RobertaWithTabular.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_type_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">position_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs_embeds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">numerical_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_transformers.html#RobertaWithTabular.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.RobertaWithTabular.forward" title="Link to this definition"></a></dt>
<dd><p>The [<cite>RobertaWithTabular</cite>] forward method, overrides the <cite>__call__</cite> special method.</p>
<p>&lt;Tip&gt;</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the [<cite>Module</cite>]
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.</p>
<p>&lt;/Tip&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<cite>AutoTokenizer</cite>]. See [<cite>PreTrainedTokenizer.encode</cite>] and
[<cite>PreTrainedTokenizer.__call__</cite>] for details.</p>
<p>[What are input IDs?](../glossary#input-ids)</p>
</p></li>
<li><p><strong>attention_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>, <em>optional</em>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p>[What are attention masks?](../glossary#attention-mask)</p>
</p></li>
<li><p><strong>token_type_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>, <em>optional</em>) – <p>Segment token indices to indicate first and second portions of the inputs. Indices are selected in <cite>[0,1]</cite>:</p>
<ul>
<li><p>0 corresponds to a <em>sentence A</em> token,</p></li>
<li><p>1 corresponds to a <em>sentence B</em> token.</p></li>
</ul>
<p>This parameter can only be used when the model is initialized with <cite>type_vocab_size</cite> parameter with value
&gt;= 2. All the value in this tensor should be always &lt; type_vocab_size.</p>
<p>[What are token type IDs?](../glossary#token-type-ids)</p>
</p></li>
<li><p><strong>position_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>, <em>optional</em>) – <p>Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <cite>[0,
config.max_position_embeddings - 1]</cite>.</p>
<p>[What are position IDs?](../glossary#position-ids)</p>
</p></li>
<li><p><strong>head_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>(num_heads,)</cite> or <cite>(num_layers, num_heads)</cite>, <em>optional</em>) – <p>Mask to nullify selected heads of the self-attention modules. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>inputs_embeds</strong> (<cite>torch.FloatTensor</cite> of shape <cite>((batch_size, sequence_length), hidden_size)</cite>, <em>optional</em>) – Optionally, instead of passing <cite>input_ids</cite> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors than the
model’s internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the attentions tensors of all attention layers. See <cite>attentions</cite> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the hidden states of all layers. See <cite>hidden_states</cite> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<cite>bool</cite>, <em>optional</em>) – <p>Whether or not to return a [<cite>~utils.ModelOutput</cite>] instead of a plain tuple.</p>
<dl class="simple">
<dt>labels (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>):</dt><dd><p>Labels for computing the sequence classification/regression loss.
Indices should be in <code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.num_labels</span> <span class="pre">-</span> <span class="pre">1]</span></code>.
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">tabular_config.num_labels</span> <span class="pre">==</span> <span class="pre">1</span></code> a regression loss is computed (Mean-Square loss),
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">tabular_config.num_labels</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> a classification loss is computed (Cross-Entropy).</p>
</dd>
<dt>cat_feats (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">tabular_config.cat_feat_dim)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>):</dt><dd><p>Categorical features to be passed in to the TabularFeatCombiner</p>
</dd>
<dt>numerical_feats (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">tabular_config.numerical_feat_dim)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>):</dt><dd><p>Numerical features to be passed in to the TabularFeatCombiner</p>
</dd>
</dl>
</p></li>
<li><p><strong>Returns</strong> – <p><code class="xref py py-obj docutils literal notranslate"><span class="pre">tuple</span></code> comprising various elements depending on configuration and inputs:
loss (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(1,)</span></code>, <cite>optional</cite>, returned when <code class="xref py py-obj docutils literal notranslate"><span class="pre">label</span></code> is provided):</p>
<blockquote>
<div><p>Classification (or regression if tabular_config.num_labels==1) loss.</p>
</div></blockquote>
<dl class="simple">
<dt>logits (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">tabular_config.num_labels)</span></code>):</dt><dd><p>Classification (or regression if tabular_config.num_labels==1) scores (before SoftMax).</p>
</dd>
<dt>classifier_layer_outputs(<code class="xref py py-obj docutils literal notranslate"><span class="pre">list</span></code> of <code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>):</dt><dd><p>The outputs of each layer of the final classification layers. The 0th index of this list is the
combining module’s output</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.XLMRobertaWithTabular">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">XLMRobertaWithTabular</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hf_model_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_transformers.html#XLMRobertaWithTabular"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.XLMRobertaWithTabular" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.RobertaWithTabular" title="multimodal_transformers.model.tabular_transformers.RobertaWithTabular"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaWithTabular</span></code></a></p>
<p>This class overrides <a class="reference internal" href="#multimodal_transformers.model.tabular_transformers.RobertaWithTabular" title="multimodal_transformers.model.tabular_transformers.RobertaWithTabular"><code class="xref py py-class docutils literal notranslate"><span class="pre">RobertaWithTabular</span></code></a>. Please check the
superclass for the appropriate documentation alongside usage examples.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.XLMRobertaWithTabular.config_class">
<span class="sig-name descname"><span class="pre">config_class</span></span><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.XLMRobertaWithTabular.config_class" title="Link to this definition"></a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">XLMRobertaConfig</span></code></p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.XLMWithTabular">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">XLMWithTabular</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hf_model_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_transformers.html#XLMWithTabular"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.XLMWithTabular" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">XLMForSequenceClassification</span></code></p>
<p>XLM Model transformer with a sequence classification/regression head as well as
a TabularFeatCombiner module to combine categorical and numerical features
with the Roberta pooled output</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>hf_model_config</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">XLMConfig</span></code>) – Model configuration class with all the parameters of the model.
This object must also have a tabular_config member variable that is a
<code class="xref py py-obj docutils literal notranslate"><span class="pre">TabularConfig</span></code> instance specifying the configs for <code class="xref py py-obj docutils literal notranslate"><span class="pre">TabularFeatCombiner</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.XLMWithTabular.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">langs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_type_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">position_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lengths</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs_embeds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">numerical_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_transformers.html#XLMWithTabular.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.XLMWithTabular.forward" title="Link to this definition"></a></dt>
<dd><p>The [<cite>XLMWithTabular</cite>] forward method, overrides the <cite>__call__</cite> special method.</p>
<p>&lt;Tip&gt;</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the [<cite>Module</cite>]
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.</p>
<p>&lt;/Tip&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>({0})</cite>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<cite>AutoTokenizer</cite>]. See [<cite>PreTrainedTokenizer.encode</cite>] and
[<cite>PreTrainedTokenizer.__call__</cite>] for details.</p>
<p>[What are input IDs?](../glossary#input-ids)</p>
</p></li>
<li><p><strong>attention_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>({0})</cite>, <em>optional</em>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p>[What are attention masks?](../glossary#attention-mask)</p>
</p></li>
<li><p><strong>langs</strong> (<cite>torch.LongTensor</cite> of shape <cite>({0})</cite>, <em>optional</em>) – <p>A parallel sequence of tokens to be used to indicate the language of each token in the input. Indices are
languages ids which can be obtained from the language names by using two conversion mappings provided in
the configuration of the model (only provided for multilingual models). More precisely, the <em>language name
to language id</em> mapping is in <cite>model.config.lang2id</cite> (which is a dictionary string to int) and the
<em>language id to language name</em> mapping is in <cite>model.config.id2lang</cite> (dictionary int to string).</p>
<p>See usage examples detailed in the [multilingual documentation](../multilingual).</p>
</p></li>
<li><p><strong>token_type_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>({0})</cite>, <em>optional</em>) – <p>Segment token indices to indicate first and second portions of the inputs. Indices are selected in <cite>[0,
1]</cite>:</p>
<ul>
<li><p>0 corresponds to a <em>sentence A</em> token,</p></li>
<li><p>1 corresponds to a <em>sentence B</em> token.</p></li>
</ul>
<p>[What are token type IDs?](../glossary#token-type-ids)</p>
</p></li>
<li><p><strong>position_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>({0})</cite>, <em>optional</em>) – <p>Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <cite>[0,
config.max_position_embeddings - 1]</cite>.</p>
<p>[What are position IDs?](../glossary#position-ids)</p>
</p></li>
<li><p><strong>lengths</strong> (<cite>torch.LongTensor</cite> of shape <cite>(batch_size,)</cite>, <em>optional</em>) – Length of each sentence that can be used to avoid performing attention on padding token indices. You can
also use <em>attention_mask</em> for the same result (see above), kept here for compatibility. Indices selected in
<cite>[0, …, input_ids.size(-1)]</cite>.</p></li>
<li><p><strong>cache</strong> (<cite>Dict[str, torch.FloatTensor]</cite>, <em>optional</em>) – <p>Dictionary string to <cite>torch.FloatTensor</cite> that contains precomputed hidden states (key and values in the
attention blocks) as computed by the model (see <cite>cache</cite> output below). Can be used to speed up sequential
decoding.</p>
<p>The dictionary object will be modified in-place during the forward pass to add newly computed
hidden-states.</p>
</p></li>
<li><p><strong>head_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>(num_heads,)</cite> or <cite>(num_layers, num_heads)</cite>, <em>optional</em>) – <p>Mask to nullify selected heads of the self-attention modules. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>inputs_embeds</strong> (<cite>torch.FloatTensor</cite> of shape <cite>({0}, hidden_size)</cite>, <em>optional</em>) – Optionally, instead of passing <cite>input_ids</cite> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors than the
model’s internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the attentions tensors of all attention layers. See <cite>attentions</cite> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the hidden states of all layers. See <cite>hidden_states</cite> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return a [<cite>~utils.ModelOutput</cite>] instead of a plain tuple.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the sequence classification/regression loss.
Indices should be in <code class="xref py py-obj docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.num_labels</span> <span class="pre">-</span> <span class="pre">1]</span></code>.
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">==</span> <span class="pre">1</span></code> a regression loss is computed (Mean-Square loss),
If <code class="xref py py-obj docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> a classification loss is computed (Cross-Entropy).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.XLNetWithTabular">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">XLNetWithTabular</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hf_model_config</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_transformers.html#XLNetWithTabular"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.XLNetWithTabular" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetForSequenceClassification</span></code></p>
<p>XLNet Model transformer with a sequence classification/regression head as well as
a TabularFeatCombiner module to combine categorical and numerical features
with the Roberta pooled output</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>hf_model_config</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">XLNetConfig</span></code>) – Model configuration class with all the parameters of the model.
This object must also have a tabular_config member variable that is a
<code class="xref py py-obj docutils literal notranslate"><span class="pre">TabularConfig</span></code> instance specifying the configs for <code class="xref py py-obj docutils literal notranslate"><span class="pre">TabularFeatCombiner</span></code></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="multimodal_transformers.model.tabular_transformers.XLNetWithTabular.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mems</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">perm_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_mapping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">token_type_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">head_mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs_embeds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_attentions</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_hidden_states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cat_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">numerical_feats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/multimodal_transformers/model/tabular_transformers.html#XLNetWithTabular.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#multimodal_transformers.model.tabular_transformers.XLNetWithTabular.forward" title="Link to this definition"></a></dt>
<dd><p>The [<cite>XLNetWithTabular</cite>] forward method, overrides the <cite>__call__</cite> special method.</p>
<p>&lt;Tip&gt;</p>
<p>Although the recipe for forward pass needs to be defined within this function, one should call the [<cite>Module</cite>]
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.</p>
<p>&lt;/Tip&gt;</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>) – <p>Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using [<cite>AutoTokenizer</cite>]. See [<cite>PreTrainedTokenizer.encode</cite>] and
[<cite>PreTrainedTokenizer.__call__</cite>] for details.</p>
<p>[What are input IDs?](../glossary#input-ids)</p>
</p></li>
<li><p><strong>attention_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>, <em>optional</em>) – <p>Mask to avoid performing attention on padding token indices. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 for tokens that are <strong>not masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>masked</strong>.</p></li>
</ul>
<p>[What are attention masks?](../glossary#attention-mask)</p>
</p></li>
<li><p><strong>mems</strong> (<cite>List[torch.FloatTensor]</cite> of length <cite>config.n_layers</cite>) – <p>Contains pre-computed hidden-states (see <cite>mems</cite> output below) . Can be used to speed up sequential
decoding. The token ids which have their past given to this model should not be passed as <cite>input_ids</cite> as
they have already been computed.</p>
<p><cite>use_mems</cite> has to be set to <cite>True</cite> to make use of <cite>mems</cite>.</p>
</p></li>
<li><p><strong>perm_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>(batch_size, sequence_length, sequence_length)</cite>, <em>optional</em>) – <p>Mask to indicate the attention pattern for each input token with values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>if <cite>perm_mask[k, i, j] = 0</cite>, i attend to j in batch k;</p></li>
<li><p>if <cite>perm_mask[k, i, j] = 1</cite>, i does not attend to j in batch k.</p></li>
</ul>
<p>If not set, each token attends to all the others (full bidirectional attention). Only used during
pretraining (to define factorization order) or for sequential decoding (generation).</p>
</p></li>
<li><p><strong>target_mapping</strong> (<cite>torch.FloatTensor</cite> of shape <cite>(batch_size, num_predict, sequence_length)</cite>, <em>optional</em>) – Mask to indicate the output tokens to use. If <cite>target_mapping[k, i, j] = 1</cite>, the i-th predict in batch k is
on the j-th token. Only used during pretraining for partial prediction or for sequential decoding
(generation).</p></li>
<li><p><strong>token_type_ids</strong> (<cite>torch.LongTensor</cite> of shape <cite>((batch_size, sequence_length))</cite>, <em>optional</em>) – <p>Segment token indices to indicate first and second portions of the inputs. Indices are selected in <cite>[0,
1]</cite>:</p>
<ul>
<li><p>0 corresponds to a <em>sentence A</em> token,</p></li>
<li><p>1 corresponds to a <em>sentence B</em> token.</p></li>
</ul>
<p>[What are token type IDs?](../glossary#token-type-ids)</p>
</p></li>
<li><p><strong>input_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>(batch_size, sequence_length)</cite>, <em>optional</em>) – <p>Mask to avoid performing attention on padding token indices. Negative of <cite>attention_mask</cite>, i.e. with 0 for
real tokens and 1 for padding which is kept for compatibility with the original code base.</p>
<p>Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 for tokens that are <strong>masked</strong>,</p></li>
<li><p>0 for tokens that are <strong>not masked</strong>.</p></li>
</ul>
<p>You can only uses one of <cite>input_mask</cite> and <cite>attention_mask</cite>.</p>
</p></li>
<li><p><strong>head_mask</strong> (<cite>torch.FloatTensor</cite> of shape <cite>(num_heads,)</cite> or <cite>(num_layers, num_heads)</cite>, <em>optional</em>) – <p>Mask to nullify selected heads of the self-attention modules. Mask values selected in <cite>[0, 1]</cite>:</p>
<ul>
<li><p>1 indicates the head is <strong>not masked</strong>,</p></li>
<li><p>0 indicates the head is <strong>masked</strong>.</p></li>
</ul>
</p></li>
<li><p><strong>inputs_embeds</strong> (<cite>torch.FloatTensor</cite> of shape <cite>((batch_size, sequence_length), hidden_size)</cite>, <em>optional</em>) – Optionally, instead of passing <cite>input_ids</cite> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <cite>input_ids</cite> indices into associated vectors than the
model’s internal embedding lookup matrix.</p></li>
<li><p><strong>output_attentions</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the attentions tensors of all attention layers. See <cite>attentions</cite> under returned
tensors for more detail.</p></li>
<li><p><strong>output_hidden_states</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return the hidden states of all layers. See <cite>hidden_states</cite> under returned tensors for
more detail.</p></li>
<li><p><strong>return_dict</strong> (<cite>bool</cite>, <em>optional</em>) – Whether or not to return a [<cite>~utils.ModelOutput</cite>] instead of a plain tuple.</p></li>
<li><p><strong>labels</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">torch.LongTensor</span></code> of shape <code class="xref py py-obj docutils literal notranslate"><span class="pre">(batch_size,)</span></code>, <cite>optional</cite>, defaults to <code class="xref py py-obj docutils literal notranslate"><span class="pre">None</span></code>) – Labels for computing the sequence classification/regression loss.
Indices should be in <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">config.num_labels</span> <span class="pre">-</span> <span class="pre">1]</span></code>.
If <code class="docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">==</span> <span class="pre">1</span></code> a regression loss is computed (Mean-Square loss),
If <code class="docutils literal notranslate"><span class="pre">config.num_labels</span> <span class="pre">&gt;</span> <span class="pre">1</span></code> a classification loss is computed (Cross-Entropy).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../notes/colab_example.html" class="btn btn-neutral float-left" title="Colab Example" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="data.html" class="btn btn-neutral float-right" title="multimodal_transformers.data" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Ken Gu.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>